{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\n\nX = np.array([1, 2, 3, 4, 5])\ny = np.array([1, 2, 3, 4, 5])\n\nX_b = np.c_[np.ones((len(X), 1)), X]\n\n#Î¸ = (X^T * X)^(-1) * X^T * y\ntheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n\nX_new = np.array([[0], [2], [3]])\nX_new_b = np.c_[np.ones((len(X_new), 1)), X_new]\ny_predict = X_new_b.dot(theta_best)\n\nprint(\"Predictions:\", y_predict)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Predictions: [2.66453526e-15 2.00000000e+00 3.00000000e+00]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef gradient_descent(X, y, learning_rate, iterations):\n    m = len(y)\n    X_b = np.c_[np.ones((m, 1)), X]\n    theta = np.random.randn(X_b.shape[1], 1)\n    \n    for i in range(iterations):\n        z = X_b.dot(theta)\n        predictions = sigmoid(z)\n        gradients = X_b.T.dot(predictions - y) / m\n        theta -= learning_rate * gradients\n    \n    return theta\n\nX = np.array([[1], [2], [3], [4]])\ny = np.array([[0], [0], [1], [1]])\n\ntheta_best = gradient_descent(X, y, learning_rate=0.1, iterations=1000)\nprint(\"Best parameters:\", theta_best)\n\nX_new = np.array([[1], [3]])\nX_new_b = np.c_[np.ones((len(X_new), 1)), X_new]\npredictions = sigmoid(X_new_b.dot(theta_best))\nprint(\"Predictions:\", predictions)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Best parameters: [[-5.81461674]\n [ 2.43270285]]\nPredictions: [[0.03286551]\n [0.81509942]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom collections import Counter\n\n# Distance function\ndef euclidean_distance(a, b):\n    return np.sqrt(np.sum((a - b)**2))\n\n# K-NN classifier\ndef knn(X_train, y_train, X_test, k=3):\n    y_pred = []\n    for test_point in X_test:\n        distances = [euclidean_distance(test_point, x) for x in X_train]\n        k_indices = np.argsort(distances)[:k]\n        k_nearest_labels = [y_train[i] for i in k_indices]\n        majority_vote = Counter(k_nearest_labels).most_common(1)[0][0]\n        y_pred.append(majority_vote)\n    return y_pred\n\n# Sample data\nX_train = np.array([[1, 1], [2, 2], [3, 3], [6, 6], [7, 7]])\ny_train = np.array([0, 0, 0, 1, 1])\nX_test = np.array([[4, 4], [5, 5]])\n\n# Run K-NN classifier\ny_pred = knn(X_train, y_train, X_test, k=3)\nprint(\"Predicted labels:\", y_pred)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Predicted labels: [0, 1]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\n# Sample data (each row is a data point)\nX = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0]])\n\n# Standardize data (zero mean and unit variance)\nX_meaned = X - np.mean(X, axis=0)\n\n# Covariance matrix\ncov_matrix = np.cov(X_meaned.T)\n\n# Eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n# Sort eigenvectors by eigenvalues in decreasing order\nidx = eigenvalues.argsort()[::-1]\neigenvectors = eigenvectors[:, idx]\n\n# Transform the data to the new basis\nX_pca = X_meaned.dot(eigenvectors)\n\nprint(\"PCA transformed data:\\n\", X_pca)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "PCA transformed data:\n [[ 0.44362444 -0.20099093]\n [-2.17719404 -0.05500992]\n [ 0.57071239  0.36808609]\n [-0.12902465  0.06747325]\n [ 1.29188186 -0.17955849]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\n# Euclidean distance\ndef euclidean_distance(a, b):\n    return np.sqrt(np.sum((a - b)**2))\n\n# K-Means algorithm\ndef kmeans(X, k, iterations):\n    # Randomly initialize centroids\n    centroids = X[np.random.choice(X.shape[0], k, replace=False)]\n    \n    for _ in range(iterations):\n        # Assign points to the nearest centroid\n        clusters = [[] for _ in range(k)]\n        for point in X:\n            distances = [euclidean_distance(point, centroid) for centroid in centroids]\n            cluster_index = np.argmin(distances)\n            clusters[cluster_index].append(point)\n        \n        # Update centroids\n        new_centroids = np.array([np.mean(cluster, axis=0) for cluster in clusters])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    \n    return centroids, clusters\n\n# Sample data\nX = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n\n# Run K-Means\ncentroids, clusters = kmeans(X, k=2, iterations=100)\nprint(\"Centroids:\\n\", centroids)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Centroids:\n [[7.33333333 9.        ]\n [1.16666667 1.46666667]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\n# Calculate probabilities\ndef calculate_probabilities(X, y):\n    unique_classes = np.unique(y)\n    probabilities = {}\n    \n    for label in unique_classes:\n        X_class = X[y == label]\n        probabilities[label] = {}\n        probabilities[label]['prior'] = len(X_class) / len(y)\n        probabilities[label]['mean'] = np.mean(X_class, axis=0)\n        probabilities[label]['var'] = np.var(X_class, axis=0)\n    \n    return probabilities\n\n# Gaussian probability\ndef gaussian_probability(x, mean, var):\n    coeff = 1 / np.sqrt(2 * np.pi * var)\n    exponent = np.exp(-(x - mean)**2 / (2 * var))\n    return coeff * exponent\n\n# Predict class label\ndef predict(X_test, probabilities):\n    predictions = []\n    for x in X_test:\n        posteriors = {}\n        for label, params in probabilities.items():\n            prior = np.log(params['prior'])\n            likelihood = np.sum(np.log(gaussian_probability(x, params['mean'], params['var'])))\n            posteriors[label] = prior + likelihood\n        predictions.append(max(posteriors, key=posteriors.get))\n    return predictions\n\n# Sample data\nX_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\ny_train = np.array([0, 0, 1, 1, 1])\nX_test = np.array([[2.5, 3.5], [3.5, 4.5]])\n\n# Run Naive Bayes classifier\nprobabilities = calculate_probabilities(X_train, y_train)\ny_pred = predict(X_test, probabilities)\nprint(\"Predictions:\", y_pred)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Predictions: [1, 1]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\n# Hinge loss function for SVM\ndef hinge_loss(X, y, w, b, reg_strength):\n    n_samples = X.shape[0]\n    distances = 1 - y * (np.dot(X, w) + b)\n    losses = np.maximum(0, distances)\n    return reg_strength * (np.sum(losses) / n_samples)\n\n# Gradient descent for SVM\ndef gradient_descent(X, y, w, b, learning_rate, iterations, reg_strength):\n    for _ in range(iterations):\n        for i, x_i in enumerate(X):\n            condition = y[i] * (np.dot(X[i], w) + b) >= 1\n            if condition:\n                w -= learning_rate * (2 * reg_strength * w)\n            else:\n                w -= learning_rate * (2 * reg_strength * w - np.dot(X[i], y[i]))\n                b -= learning_rate * y[i]\n    return w, b\n\n# Sample data\nX = np.array([[1, 2], [2, 3], [3, 3], [4, 5], [5, 6]])\ny = np.array([1, 1, -1, -1, -1])\n\n# Parameters\nw = np.zeros(X.shape[1])\nb = 0\nlearning_rate = 0.001\niterations = 1000\nreg_strength = 0.01\n\n# Train SVM\nw, b = gradient_descent(X, y, w, b, learning_rate, iterations, reg_strength)\nprint(\"Weight vector:\", w)\nprint(\"Bias term:\", b)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Weight vector: [-0.72999255  0.57392065]\nBias term: -0.9520000000000007\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}